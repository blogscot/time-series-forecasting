---
title: "Time Series"
author: "Iain Diamond"
format: html
---

# Chapter 7

```{r}
library(fpp3)
library(plotly)
```

```{r}
 us_data <- us_change |> 
  select(Consumption, Income, Savings) |> 
  filter_index("1980 Q1" ~ "2000 Q1")

autoplot(us_data, scale(Consumption), color = "Black") +
  autolayer(us_data, scale(Income), color = "Orange") +
  autolayer(us_data, scale(Savings), color = "Green")
```


## Exercise 1

```{r}
jan14_vic_elec <- vic_elec |>
  filter(yearmonth(Time) == yearmonth("2014 Jan")) |>
  index_by(Date = as_date(Time)) |>
  summarise(
    Demand = sum(Demand),
    Temperature = max(Temperature)
  )
jan14_vic_elec
```
### a 

Plot the data and find the regression model for Demand with temperature as a predictor variable. Why is there a positive relationship?

```{r}
autoplot(jan14_vic_elec, scale(Demand), color = "purple") +
  autolayer(jan14_vic_elec, scale(Temperature), colour = "orange") +
  labs(y = "Demand (Purple), Temp (Orange)")
  
```

```{r}
g <- jan14_vic_elec |>
  ggplot(aes(x = Temperature, y = Demand)) +
  labs(y = "Victoria Half-hourly Electricity Demand",
       x = "Temperature") +
  geom_point() +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE)

ggplotly(g)
```
```{r}

```



To a large extent the energy demand tracks the temperature, which suggests australians are using energy to keep themselves cool using air-conditioning and in turn exacerbating the climate crisis.

```{r}
jan14_vic_elec |> 
  CCF(Demand, Temperature) |> autoplot() +
  labs(y = "Cross Correlation Demand versus Temperature")
```
As an aside: this cross-correlation plot shows that there is a strong cross correlation for lag -1 and lag 1, that is, the energy consumption yesterday is similar to today, and equally the energy demand tomorrow will be similar to today.

### b 

Produce a residual plot. Is the model adequate? Are there any outliers or influential observations?

```{r}
jan14_vic_elec_fit <- jan14_vic_elec |>
  model(TSLM(Demand ~ Temperature))

jan14_vic_elec_fit |> report()
```


```{r}
jan14_vic_elec_fit |> gg_tsresiduals()
```
The ACF plot how that the residuals have no autocorrelation issue. The histogram has zero mean but it hardly looks bell-shaped.

```{r}
library(rstatix)

residuals <- augment(jan14_vic_elec_fit) |> 
  pull(.resid)

shapiro_test(residuals)
```
The null hypothesis for the Shapiro-Wilks test is that the data is normally distribution. As the p-value is not less than the value of 0.05 we do not reject the null hypothesis. Hence the data distribution is in fact normal.

```{r}
car::qqPlot(residuals)
```


### c 

Use the model to forecast the electricity demand that you would expect for the next day if the maximum temperature was 15˚C and compare it with the forecast if the with maximum temperature was 35˚C. Do you believe these forecasts?

```{r}

jan14_vic_elec_fit <- jan14_vic_elec |>
  model(TSLM(Demand ~ Temperature)) 

jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 15)
  ) |>
  autoplot(jan14_vic_elec) +
  jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 35)
  ) |> 
  autolayer(jan14_vic_elec)
```
```{r}
jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 15)
  )
```




```{r}
estimates <- jan14_vic_elec_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")

paste("Predicted demand at 15 degrees = ", intercept + 15 * slope)
paste("Predicted demand at 35 degrees = ", intercept + 35 * slope)
```

Using the intercept and slope from the fitted model, shown above, the forecast electricity demand for 15 and 35 degrees do correspond with values shown in the time plot.

Just for the sake of curiosity let's examine the residuals against the predictors.

```{r}
jan14_vic_elec |>
  left_join(residuals(jan14_vic_elec_fit), by = "Date") |>
  pivot_longer(Demand:Temperature,
               names_to = "regressor", values_to = "x") |>
  ggplot(aes(x = x, y = .resid)) +
  geom_point() +
  facet_wrap(. ~ regressor, scales = "free_x") +
  labs(y = "Residuals", x = "")
```
No patterns in these plots. Good stuff!

### d 

Give prediction intervals for your forecasts.

```{r}
get_prediction_intervals <- function(mean, sd) {
  tibble("80% interval" = c(qnorm(0.1, mean=mean, sd=sd), 
                            qnorm(0.9, mean=mean, sd=sd)),
         "95% interval" = c(qnorm(0.025, mean=mean, sd=sd), 
         qnorm(0.975, mean=mean, sd=sd)))
}
```


```{r}
jan14_vic_elec_fc <- jan14_vic_elec_fit |> 
  forecast(new_data(jan14_vic_elec, 2) |> 
             mutate(Temperature = c(15, 35)))
jan14_vic_elec_fc
```

```{r}
jan14_vic_elec_fc  |> 
  hilo() |> 
  select(`80%`, `95%`)
```
I initially had forgotten about the `hilo` function and calculated the prediction intervals the hard way, see below. The results are close (e.g 117908.1 versus 117979.6) but not exact. My assumption is that the mean and standard deviation that I copied by hand are rounded values which is the root of these discrepancies.

```{r}
mean = 151398.4
sd = sqrt(6.8e+08)
x <- seq(mean-sd*4, mean+sd*4)
y <- dnorm(x, mean = mean, sd=sd)

plot(x,y, type = "l", lwd = 2, xlab = "x", ylab = "Frequency")
get_prediction_intervals(mean, sd)
```


```{r}
mean = 274484
sd = sqrt(6.4e+08)
x <- seq(mean-sd*4, mean+sd*4)
y <- dnorm(x, mean = mean, sd=sd)

plot(x,y, type = "l", lwd = 2, xlab = "x", ylab = "Frequency")
get_prediction_intervals(mean, sd)
```

### e 

Plot Demand vs Temperature for all of the available data in vic_elec aggregated to daily total demand and maximum temperature. What does this say about your model?

```{r}
vic_elec |> 
  index_by(Date) |> 
  summarise(totalDemand = sum(Demand), maxTemp = max(Temperature)) |> 
  ggplot(aes(x = maxTemp, y = totalDemand)) +
  geom_point() +
  labs(title = "Australian Energy Usage", 
       x = "Maximum Temperature (˚C)", 
       y = "Total Energy Demand (MWh)")
```
The plot shows that the relationship between totalDemand and maxTemp is non-linear. As the temperature increases between 10 and 20 degrees energy demand decreases perhaps because consumers use less energy on heating. As the temperature rises above 25 degrees energy usage rises perhaps because consumers are using air-conditioning to cool their homes and offices.

The model uses data taken from January 14 which is the middle of Australia's summer, thus the model doesn't cover the energy demand during the cooler periods of the year, hence it's possible to fit a linear model to the data.

## Exercise 2

Data set olympic_running contains the winning times (in seconds) in each Olympic Games sprint, middle-distance and long-distance track events from 1896 to 2016.


### a

Plot the winning time against the year for each event. Describe the main features of the plot.

```{r}
olympic_records <- olympic_running |> 
  drop_na(Time) |> 
  group_by(Length, Sex) |> 
  filter(Time == min(Time)) |> 
  ungroup()
olympic_records
```
```{r}
#| warning: false
#| message: false

g <- olympic_records |> 
  mutate(Event = paste(Length, Sex)) |> 
  ggplot(aes(y = Year, x = Time, colour = Event)) +
  geom_point() +
  scale_y_continuous(breaks=seq(1980, 2016, 4)) +
  labs(title = "New Olympic Records (Running)", x = "Time (s)")
ggplotly(g)
```
For shorter track distances the women's Olympic records haven't been broken in decades, while for the longer track distances records have been broken fairly recently. 

The plot is now interactive and the y-axis increments by 4 to match the Olympic event years. 

### b

Fit a regression line to the data for each event. Obviously the winning times have been decreasing, but at what average rate per year?

```{r}
olympic_running |> mutate(Event = paste(Length, Sex)) |> distinct(Event)
```
```{r}
#| warning: false
olympic_running |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Olympic Games times")
```
```{r}

# Let's fit all the events all at once!
olympic_events_fit <- olympic_running |> 
  mutate(olympics_num = row_number()) |> 
  update_tsibble(key = c(Length, Sex), index = olympics_num, regular = TRUE) |> 
  model(TSLM(Year ~ Time))

# We can examine individual event models by filtering
olympic_events_fit |> 
  filter(Length == "10000", Sex == 'men') |> 
  report()
```
```{r}
olympic_events_fit |> 
  filter(Length == "10000", Sex == 'women') |> 
  report()
```

```{r}
#| warning: false

olympic_running |> 
  filter(Length == "10000", Sex == "men") |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Men's 10000m Olympic Games times")
```


```{r}
olympic_10000_mens_event_fit <- olympic_events_fit |> 
  filter(Length == "10000", Sex == 'men')

estimates <- olympic_10000_mens_event_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")
```

```{r}
#| warning: false

olympic_running |> 
  filter(Length == "10000", Sex == "women") |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Women's 10000m Olympic Games times")
```

```{r}
olympic_10000_womens_event_fit <- olympic_events_fit |> 
  filter(Length == "10000", Sex == 'women')

estimates <- olympic_10000_womens_event_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")
```

Men started competing in the Olympics 10,000m at the start of the 20th century. Their times are, perhaps surprisingly, slower than the women's times who only began competing far more recently in the mid 1980s. Clearly, the culture of modern athleticism has advanced greatly since the start of the 20th century for both sexes. 

Due to the initially relatively slow men's Olympic times at 10,000m distances their average rate of improvement is greater, at 0.34, than that of the women's average rate of improvement, at 0.23.

### c

Plot the residuals against the year. What does this indicate about the suitability of the fitted lines?

```{r}
#| warning: false

olympic_10000_mens_event_fit |> gg_tsresiduals()
```

The residuals for the men's 10,000m are fairly small, there is no significant autocorrelation but there is a negitive bias in the results. Is this the median figure of -2.172?
```{r}
#| warning: false

olympic_10000_womens_event_fit |> gg_tsresiduals()
```
The women's 10,000m residuals have fewer points; they are very small indicating that the model has a good accuracy. There is no appreciable autocorrelation. Lastly, the histogram is very roughly normal (it's hard with few so few data points) and there is a small negitive bias: -0.9872.

### d

Predict the winning time for each race in the 2020 Olympics. Give a prediction interval for your forecasts. What assumptions have you made in these calculations?

```{r}
olympic_events_trends_fit <- olympic_running |> 
  model(TSLM(Time ~ trend()))

olympic_events_trends_fc <- olympic_events_trends_fit |> forecast()
olympic_events_trends_fc
```

```{r}
men_10000 <- olympic_running |>
  filter(Length == "10000", Sex == "men")

olympic_events_trends_fc |>
    filter(Length == "10000", Sex == 'men') |> 
  autoplot(men_10000)

olympic_events_trends_fc |> 
  filter(Length == "10000", Sex == 'men') |> 
  hilo()
```
```{r}
women_10000 <- olympic_running |>
  filter(Length == "10000", Sex == "women")

olympic_events_trends_fc |>
    filter(Length == "10000", Sex == 'women') |> 
  autoplot(women_10000)

olympic_events_trends_fc |> 
  filter(Length == "10000", Sex == 'women') |> 
  hilo()
```
Both these predictions are linear meaning that according to the model eventually mankind will be able to travel distances in zero seconds, which is impossible. Hence, better models should be used to capture the point where human performance start to plateau.

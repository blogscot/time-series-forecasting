---
title: "Time Series"
author: "Iain Diamond"
format: html
---

For when I get stuck [Dan McSwain Solutions](https://rstudio-pubs-static.s3.amazonaws.com/834769_a7e343c5024745eeb06f9c6031d0c6b6.html)

# Chapter 7

```{r}
library(fpp3)
library(plotly)
```

## Exercise 1

```{r}
jan14_vic_elec <- vic_elec |>
  filter(yearmonth(Time) == yearmonth("2014 Jan")) |>
  index_by(Date = as_date(Time)) |>
  summarise(
    Demand = sum(Demand),
    Temperature = max(Temperature)
  )
jan14_vic_elec
```

### a

Plot the data and find the regression model for Demand with temperature as a predictor variable. Why is there a positive relationship?

```{r}
autoplot(jan14_vic_elec, scale(Demand), color = "purple") +
  autolayer(jan14_vic_elec, scale(Temperature), colour = "orange") +
  labs(y = "Demand (Purple), Temp (Orange)")
  
```

```{r}
g <- jan14_vic_elec |>
  ggplot(aes(x = Temperature, y = Demand)) +
  labs(y = "Victoria Half-hourly Electricity Demand",
       x = "Temperature") +
  geom_point() +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE)

ggplotly(g)
```

To a large extent the energy demand tracks the temperature, which suggests australians are using energy to keep themselves cool using air-conditioning and in turn exacerbating the climate crisis.

```{r}
jan14_vic_elec |> 
  CCF(Demand, Temperature) |> autoplot() +
  labs(y = "Cross Correlation Demand versus Temperature")
```

As an aside: this cross-correlation plot shows that there is a strong cross correlation for lag -1 and lag 1, that is, the energy consumption yesterday is similar to today, and equally the energy demand tomorrow will be similar to today.

### b

Produce a residual plot. Is the model adequate? Are there any outliers or influential observations?

```{r}
jan14_vic_elec_fit <- jan14_vic_elec |>
  model(TSLM(Demand ~ Temperature))

jan14_vic_elec_fit |> report()
```

```{r}
jan14_vic_elec_fit |> gg_tsresiduals()
```

The ACF plot how that the residuals have no autocorrelation issue. The histogram has zero mean but it hardly looks bell-shaped.

```{r}
library(rstatix)

residuals <- augment(jan14_vic_elec_fit) |> 
  pull(.resid)

shapiro_test(residuals)
```

The null hypothesis for the Shapiro-Wilks test is that the data is normally distribution. As the p-value is not less than the value of 0.05 we do not reject the null hypothesis. Hence the data distribution is in fact normal.

```{r}
car::qqPlot(residuals)
```

### c

Use the model to forecast the electricity demand that you would expect for the next day if the maximum temperature was 15˚C and compare it with the forecast if the with maximum temperature was 35˚C. Do you believe these forecasts?

```{r}

jan14_vic_elec_fit <- jan14_vic_elec |>
  model(TSLM(Demand ~ Temperature)) 

jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 15)
  ) |>
  autoplot(jan14_vic_elec) +
  jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 35)
  ) |> 
  autolayer(jan14_vic_elec)
```

```{r}
jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 15)
  )
```

```{r}
estimates <- jan14_vic_elec_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")

paste("Predicted demand at 15 degrees = ", intercept + 15 * slope)
paste("Predicted demand at 35 degrees = ", intercept + 35 * slope)
```

Using the intercept and slope from the fitted model, shown above, the forecast electricity demand for 15 and 35 degrees do correspond with values shown in the time plot.

Just for the sake of curiosity let's examine the residuals against the predictors.

```{r}
jan14_vic_elec |>
  left_join(residuals(jan14_vic_elec_fit), by = "Date") |>
  pivot_longer(Demand:Temperature,
               names_to = "regressor", values_to = "x") |>
  ggplot(aes(x = x, y = .resid)) +
  geom_point() +
  facet_wrap(. ~ regressor, scales = "free_x") +
  labs(y = "Residuals", x = "")
```

No patterns in these plots. Good stuff!

### d

Give prediction intervals for your forecasts.

```{r}
get_prediction_intervals <- function(mean, sd) {
  tibble("80% interval" = c(qnorm(0.1, mean=mean, sd=sd), 
                            qnorm(0.9, mean=mean, sd=sd)),
         "95% interval" = c(qnorm(0.025, mean=mean, sd=sd), 
         qnorm(0.975, mean=mean, sd=sd)))
}
```

```{r}
jan14_vic_elec_fc <- jan14_vic_elec_fit |> 
  forecast(new_data(jan14_vic_elec, 2) |> 
             mutate(Temperature = c(15, 35)))
jan14_vic_elec_fc
```

```{r}
jan14_vic_elec_fc  |> 
  hilo() |> 
  select(`80%`, `95%`)
```

I initially had forgotten about the `hilo` function and calculated the prediction intervals the hard way, see below. The results are close (e.g 117908.1 versus 117979.6) but not exact. My assumption is that the mean and standard deviation that I copied by hand are rounded values which is the root of these discrepancies.

```{r}
mean = 151398.4
sd = sqrt(6.8e+08)
x <- seq(mean-sd*4, mean+sd*4)
y <- dnorm(x, mean = mean, sd=sd)

plot(x,y, type = "l", lwd = 2, xlab = "x", ylab = "Frequency")
get_prediction_intervals(mean, sd)
```

```{r}
mean = 274484
sd = sqrt(6.4e+08)
x <- seq(mean-sd*4, mean+sd*4)
y <- dnorm(x, mean = mean, sd=sd)

plot(x,y, type = "l", lwd = 2, xlab = "x", ylab = "Frequency")
get_prediction_intervals(mean, sd)
```

### e

Plot Demand vs Temperature for all of the available data in vic_elec aggregated to daily total demand and maximum temperature. What does this say about your model?

```{r}
vic_elec |> 
  index_by(Date) |> 
  summarise(totalDemand = sum(Demand), maxTemp = max(Temperature)) |> 
  ggplot(aes(x = maxTemp, y = totalDemand)) +
  geom_point() +
  labs(title = "Australian Energy Usage", 
       x = "Maximum Temperature (˚C)", 
       y = "Total Energy Demand (MWh)")
```

The plot shows that the relationship between totalDemand and maxTemp is non-linear. As the temperature increases between 10 and 20 degrees energy demand decreases perhaps because consumers use less energy on heating. As the temperature rises above 25 degrees energy usage rises perhaps because consumers are using air-conditioning to cool their homes and offices.

The model uses data taken from January 14 which is the middle of Australia's summer, thus the model doesn't cover the energy demand during the cooler periods of the year, hence it's possible to fit a linear model to the data.

## Exercise 2

Data set olympic_running contains the winning times (in seconds) in each Olympic Games sprint, middle-distance and long-distance track events from 1896 to 2016.

### a

Plot the winning time against the year for each event. Describe the main features of the plot.

```{r}
olympic_records <- olympic_running |> 
  drop_na(Time) |> 
  group_by(Length, Sex) |> 
  filter(Time == min(Time)) |> 
  ungroup()
olympic_records
```

```{r}
#| warning: false
#| message: false

g <- olympic_records |> 
  mutate(Event = paste(Length, Sex)) |> 
  ggplot(aes(y = Year, x = Time, colour = Event)) +
  geom_point() +
  scale_y_continuous(breaks=seq(1980, 2016, 4)) +
  labs(title = "New Olympic Records (Running)", x = "Time (s)")
ggplotly(g)
```

For shorter track distances the women's Olympic records haven't been broken in decades, while for the longer track distances records have been broken fairly recently.

The plot is now interactive and the y-axis increments by 4 to match the Olympic event years.

### b

Fit a regression line to the data for each event. Obviously the winning times have been decreasing, but at what average rate per year?

```{r}
olympic_running |> mutate(Event = paste(Length, Sex)) |> distinct(Event)
```

```{r}
#| warning: false
olympic_running |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Olympic Games times")
```

```{r}

# Let's fit all the events all at once!
olympic_events_fit <- olympic_running |> 
  mutate(olympics_num = row_number()) |> 
  update_tsibble(key = c(Length, Sex), index = olympics_num, regular = TRUE) |> 
  model(TSLM(Year ~ Time))

# We can examine individual event models by filtering
olympic_events_fit |> 
  filter(Length == "10000", Sex == 'men') |> 
  report()
```

```{r}
olympic_events_fit |> 
  filter(Length == "10000", Sex == 'women') |> 
  report()
```

```{r}
#| warning: false

olympic_running |> 
  filter(Length == "10000", Sex == "men") |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Men's 10000m Olympic Games times")
```

```{r}
olympic_10000_mens_event_fit <- olympic_events_fit |> 
  filter(Length == "10000", Sex == 'men')

estimates <- olympic_10000_mens_event_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")
```

```{r}
#| warning: false

olympic_running |> 
  filter(Length == "10000", Sex == "women") |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Women's 10000m Olympic Games times")
```

```{r}
olympic_10000_womens_event_fit <- olympic_events_fit |> 
  filter(Length == "10000", Sex == 'women')

estimates <- olympic_10000_womens_event_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")
```

Men started competing in the Olympics 10,000m at the start of the 20th century. Their times are, perhaps surprisingly, slower than the women's times who only began competing far more recently in the mid 1980s. Clearly, the culture of modern athleticism has advanced greatly since the start of the 20th century for both sexes.

Due to the initially relatively slow men's Olympic times at 10,000m distances their average rate of improvement is greater, at 0.34, than that of the women's average rate of improvement, at 0.23.

### c

Plot the residuals against the year. What does this indicate about the suitability of the fitted lines?

```{r}
#| warning: false

olympic_10000_mens_event_fit |> gg_tsresiduals()
```

The residuals for the men's 10,000m are fairly small, there is no significant autocorrelation but there is a negitive bias in the results. Is this the median figure of -2.172?

```{r}
#| warning: false

olympic_10000_womens_event_fit |> gg_tsresiduals()
```

The women's 10,000m residuals have fewer points; they are very small indicating that the model has a good accuracy. There is no appreciable autocorrelation. Lastly, the histogram is very roughly normal (it's hard with few so few data points) and there is a small negitive bias: -0.9872.

### d

Predict the winning time for each race in the 2020 Olympics. Give a prediction interval for your forecasts. What assumptions have you made in these calculations?

```{r}
olympic_events_trends_fit <- olympic_running |> 
  model(TSLM(Time ~ trend()))

olympic_events_trends_fc <- olympic_events_trends_fit |> forecast()
olympic_events_trends_fc
```

```{r}
men_10000 <- olympic_running |>
  filter(Length == "10000", Sex == "men")

olympic_events_trends_fc |>
    filter(Length == "10000", Sex == 'men') |> 
  autoplot(men_10000)

olympic_events_trends_fc |> 
  filter(Length == "10000", Sex == 'men') |> 
  hilo()
```

```{r}
women_10000 <- olympic_running |>
  filter(Length == "10000", Sex == "women")

olympic_events_trends_fc |>
    filter(Length == "10000", Sex == 'women') |> 
  autoplot(women_10000)

olympic_events_trends_fc |> 
  filter(Length == "10000", Sex == 'women') |> 
  hilo()
```

Both these predictions are linear meaning that according to the model eventually mankind will be able to travel distances in zero seconds, which is impossible. Hence, better models should be used to capture the point where human performance start to plateau.

## Exercise 3

An elasticity coefficient is the ratio of the percentage change in the forecast variable (y) to the percentage change in the predictor variable (x). Mathematically, the elasticity is defined as (dy/dx)×(x/y). Consider the log-log model,

$$
log \ y=β_0+β_1 \ log \ x+ε
$$

Express y as a function of x and show that the coefficient $β_1$ is the elasticity coefficient.

Taking the derivative of both sides with respect to $x$:

$$
\frac{d}{dx} \ log \ y = \beta_1 \frac{d}{dx} \ log \ x
$$ 
Which becomes:

$$
\frac{1}{y} \ \frac{dy}{dx} = \beta_1 \frac{1}{x}
$$
hence, we can write:

$$
\beta_1 = \frac{dy}{dx} \ \frac{x}{y} 
$$
which is the elasticity coefficient.

## Exercise 4

The data set souvenirs concerns the monthly sales figures of a shop which opened in January 1987 and sells gifts, souvenirs, and novelties. The shop is situated on the wharf at a beach resort town in Queensland, Australia. The sales volume varies with the seasonal population of tourists. There is a large influx of visitors to the town at Christmas and for the local surfing festival, held every March since 1988. Over time, the shop has expanded its premises, range of products, and staff.


```{r}
souvenirs |> head()
```


### a

Produce a time plot of the data and describe the patterns in the graph. Identify any unusual or unexpected fluctuations in the time series.

```{r}
souvenirs |> autoplot(Sales) +
  geom_smooth(formula = "y ~ x", method = "loess", se = FALSE)
```

The time plot shows that since around mid 1990 the shop has started to experience exponential growth.

## b

Explain why it is necessary to take logarithms of these data before fitting a model.

One of the assumptions of linear modelling is that the variance is constant. Clearly this is not the case with the sales data. Using a log transformation of the data helps reduce this invariance issue.

I could have used the Box-Cox transformation to remodel the data, however, as the lambda value is very close to zero it's easier to use a log transformation directly.

```{r}
lambda <- souvenirs |>
  features(Sales, features = guerrero) |>
  pull(lambda_guerrero)
lambda
```


```{r}
value <- 100
# log transformation
paste(log(value))
# box-cox transformation
paste((value^lambda - 1) / lambda)
```
It's not exact but it's close enough.

```{r}
souvenirs |> autoplot(log(Sales))
```

After the log transformation the variance of the sales data now appears constant.

```{r}
dcmp <- souvenirs |>
  model(
    STL(log(Sales) ~ trend(window = 4) + season(window = 12),
    robust = FALSE)) |>
  components() 
dcmp |> autoplot()
```

Transforming the sales data has helped regularise STL decomposition: the remainder resembles faint white noise; season_year has a regular pattern and the trend gradually increases linearly. 

If you use STL decomposition to forecast future sales we end up with the following plot. Notice, how the point forecast continues the same basic pattern as the earlier sales data. The confidence intervals, however, are quite extreme.

```{r}
fit_dcmp <- souvenirs |>
  model(stlf = decomposition_model(
    STL(log(Sales) ~ trend(window = 4) + season(window = 12), robust = FALSE),
    NAIVE(season_adjust)
  ))

fit_dcmp |>
  forecast() |>
  autoplot(souvenirs)
```


## c

Fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a “surfing festival” dummy variable.

```{r}
new_data <- souvenirs |>
  mutate(festival = ifelse(month(Month) == 3 & year(Month) > 1987, 1, 0))
new_data
```

```{r}
fit <- new_data |>
  model(TSLM(log(Sales) ~ trend() + season() + festival))
report(fit)
```

Including the festival dummy variable increased the `Adjusted R-squared` figure from 0.9447 to 0.9487.

To produce a forecast with the surfing festival dummy variable you need to provide new data covering the time period which includes values for the dummy variable.
```{r}
festival_dates <- rep(0,36)
festival_dates[seq(3,36,12)] = 1
x <- seq(as.Date("1994-01-01"), as.Date("1996-12-31"), by = "1 month")
festival_data <- tsibble(Month = yearmonth(x), festival = dates, index = Month)
festival_data
```


```{r}
forecast(fit, festival_data) |> 
  autoplot(souvenirs)
```

```{r}
fit |> gg_tsresiduals()
```

## d

Plot the residuals against time and against the fitted values. Do these plots reveal any problems with the model?

Using this `left_join` method shows the transformed residuals - the scale is between -0.4 to 0.4.

```{r}
souvenirs |>
  left_join(residuals(fit), by = "Month") |>
  ggplot(aes(x = Month, y = .resid)) +
  geom_point() 
```
Alternatively, this way shows the untransformed residuals. Scale -10,000 to 10,000. The transformed residuals appear random whereas there's an obvious pattern in the untransformed residuals.

```{r}
augment(fit) |> 
  ggplot(aes(x=Month)) +
  geom_point(aes( y=.resid))
```

The fitted values against the residuals looks to be very clustered with a handful of outliers which would indicate that the modelling assumptions have not been satisfied.

```{r}
augment(fit) |> 
  ggplot(aes(x=.fitted)) +
  geom_point(aes(y=.resid))
```
## e

Do boxplots of the residuals for each month. Does this reveal any problems with the model?

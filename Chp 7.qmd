---
title: "Time Series"
author: "Iain Diamond"
format: 
  html:
    code-fold: true
---

For when I get stuck [Dan McSwain Solutions](https://rstudio-pubs-static.s3.amazonaws.com/834769_a7e343c5024745eeb06f9c6031d0c6b6.html)

# Chapter 7

```{r}
library(fpp3)
library(plotly)
```

## Exercise 1

```{r}
jan14_vic_elec <- vic_elec |>
  filter(yearmonth(Time) == yearmonth("2014 Jan")) |>
  index_by(Date = as_date(Time)) |>
  summarise(
    Demand = sum(Demand),
    Temperature = max(Temperature)
  )
jan14_vic_elec
```

### a

Plot the data and find the regression model for Demand with temperature as a predictor variable. Why is there a positive relationship?

```{r}
autoplot(jan14_vic_elec, scale(Demand), color = "purple") +
  autolayer(jan14_vic_elec, scale(Temperature), colour = "orange") +
  labs(y = "Demand (Purple), Temp (Orange)")
  
```

```{r}
g <- jan14_vic_elec |>
  ggplot(aes(x = Temperature, y = Demand)) +
  labs(y = "Victoria Half-hourly Electricity Demand",
       x = "Temperature") +
  geom_point() +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE)

ggplotly(g)
```

To a large extent the energy demand tracks the temperature, which suggests Australians are using energy to keep themselves cool using air-conditioning and in turn exacerbating the climate crisis.

```{r}
jan14_vic_elec |> 
  CCF(Demand, Temperature) |> autoplot() +
  labs(y = "Cross Correlation Demand versus Temperature")
```

As an aside: this cross-correlation plot shows that there is a strong cross correlation for lag -1 and lag 1, that is, the energy consumption yesterday is similar to today, and equally the energy demand tomorrow will be similar to today.

### b

Produce a residual plot. Is the model adequate? Are there any outliers or influential observations?

```{r}
jan14_vic_elec_fit <- jan14_vic_elec |>
  model(TSLM(Demand ~ Temperature))

jan14_vic_elec_fit |> report()
```

```{r}
jan14_vic_elec_fit |> gg_tsresiduals()
```

The residuals show an upward trend suggesting that the model doesn't fully explain all the data. The ACF plot shows that the residuals have no autocorrelation issues. The histogram has zero mean but it hardly looks bell-shaped.

```{r}
library(rstatix)

residuals <- augment(jan14_vic_elec_fit) |> 
  pull(.resid)

shapiro_test(residuals)
```

The null hypothesis for the Shapiro-Wilks test is that the data is normally distribution. If the p-value is less than the threshold value 0.05 (e.g. a 5% chance) we reject the null hypothesis.

As the p-value is not less than the value of 0.05 we do not reject the null hypothesis. Hence the data distribution is in fact normal.

```{r}
car::qqPlot(residuals)
```

### c

Use the model to forecast the electricity demand that you would expect for the next day if the maximum temperature was 15˚C and compare it with the forecast if the with maximum temperature was 35˚C. Do you believe these forecasts?

```{r}

jan14_vic_elec_fit <- jan14_vic_elec |>
  model(TSLM(Demand ~ Temperature)) 

jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 15)
  ) |>
  autoplot(jan14_vic_elec) +
  jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 35)
  ) |> 
  autolayer(jan14_vic_elec)
```

```{r}
jan14_vic_elec_fit |>
  forecast(
    new_data(jan14_vic_elec, 1) |>
      mutate(Temperature = 15)
  )
```

```{r}
estimates <- jan14_vic_elec_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")

paste("Predicted demand at 15 degrees = ", intercept + 15 * slope)
paste("Predicted demand at 35 degrees = ", intercept + 35 * slope)
```

Using the intercept and slope from the fitted model, shown above, the forecast electricity demand for 15 and 35 degrees do correspond with values shown in the time plot.

Just for the sake of curiosity let's examine the residuals against the predictors.

```{r}
jan14_vic_elec |>
  left_join(residuals(jan14_vic_elec_fit), by = "Date") |>
  pivot_longer(Demand:Temperature,
               names_to = "regressor", values_to = "x") |>
  ggplot(aes(x = x, y = .resid)) +
  geom_point() +
  facet_wrap(. ~ regressor, scales = "free_x") +
  labs(y = "Residuals", x = "")
```

No patterns in these plots. Good stuff!

### d

Give prediction intervals for your forecasts.

```{r}
get_prediction_intervals <- function(mean, sd) {
  tibble("80% interval" = c(qnorm(0.1, mean=mean, sd=sd), 
                            qnorm(0.9, mean=mean, sd=sd)),
         "95% interval" = c(qnorm(0.025, mean=mean, sd=sd), 
         qnorm(0.975, mean=mean, sd=sd)))
}
```

```{r}
jan14_vic_elec_fc <- jan14_vic_elec_fit |> 
  forecast(new_data(jan14_vic_elec, 2) |> 
             mutate(Temperature = c(15, 35)))
jan14_vic_elec_fc
```

```{r}
jan14_vic_elec_fc  |> 
  hilo() |> 
  select(`80%`, `95%`)
```

I initially had forgotten about the `hilo` function and calculated the prediction intervals the hard way, see below. The results are close (e.g 117908.1 versus 117979.6) but not exact. My assumption is that the mean and standard deviation that I copied by hand are rounded values which is the root of these discrepancies.

```{r}
mean = 151398.4
sd = sqrt(6.8e+08)
x <- seq(mean-sd*4, mean+sd*4)
y <- dnorm(x, mean = mean, sd=sd)

plot(x,y, type = "l", lwd = 2, xlab = "x", ylab = "Frequency")
get_prediction_intervals(mean, sd)
```

```{r}
mean = 274484
sd = sqrt(6.4e+08)
x <- seq(mean-sd*4, mean+sd*4)
y <- dnorm(x, mean = mean, sd=sd)

plot(x,y, type = "l", lwd = 2, xlab = "x", ylab = "Frequency")
get_prediction_intervals(mean, sd)
```

### e

Plot Demand vs Temperature for all of the available data in vic_elec aggregated to daily total demand and maximum temperature. What does this say about your model?

```{r}
vic_elec |> 
  index_by(Date) |> 
  summarise(totalDemand = sum(Demand), maxTemp = max(Temperature)) |> 
  ggplot(aes(x = maxTemp, y = totalDemand)) +
  geom_point() +
  labs(title = "Australian Energy Usage", 
       x = "Maximum Temperature (˚C)", 
       y = "Total Energy Demand (MWh)")
```

The plot shows that the relationship between totalDemand and maxTemp is non-linear. As the temperature increases between 10 and 20 degrees energy demand decreases perhaps because consumers use less energy on heating. As the temperature rises above 25 degrees energy usage rises perhaps because consumers are using air-conditioning to cool their homes and offices.

The model uses data taken from January 14 which is the middle of Australia's summer, thus the model doesn't cover the energy demand during the cooler periods of the year, hence it's possible to fit a linear model to the data.

## Exercise 2

Data set olympic_running contains the winning times (in seconds) in each Olympic Games sprint, middle-distance and long-distance track events from 1896 to 2016.

### a

Plot the winning time against the year for each event. Describe the main features of the plot.

```{r}
olympic_records <- olympic_running |> 
  drop_na(Time) |> 
  group_by(Length, Sex) |> 
  filter(Time == min(Time)) |> 
  ungroup()
olympic_records
```

```{r}
#| warning: false
#| message: false

g <- olympic_records |> 
  mutate(Event = paste(Length, Sex)) |> 
  ggplot(aes(y = Year, x = Time, colour = Event)) +
  geom_point() +
  scale_y_continuous(breaks=seq(1980, 2016, 4)) +
  labs(title = "New Olympic Records (Running)", x = "Time (s)")
ggplotly(g)
```

For shorter track distances the women's Olympic records haven't been broken in decades, while for the longer track distances records have been broken fairly recently.

The plot is now interactive and the y-axis increments by 4 to match the Olympic event years.

### b

Fit a regression line to the data for each event. Obviously the winning times have been decreasing, but at what average rate per year?

```{r}
olympic_running |> mutate(Event = paste(Length, Sex)) |> distinct(Event)
```

```{r}
#| warning: false
olympic_running |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Olympic Games times")
```

```{r}

# Let's fit all the events all at once!
olympic_events_fit <- olympic_running |> 
  mutate(olympics_num = row_number()) |> 
  update_tsibble(key = c(Length, Sex), index = olympics_num, regular = TRUE) |> 
  model(TSLM(Year ~ Time))

# We can examine individual event models by filtering
olympic_events_fit |> 
  filter(Length == "10000", Sex == 'men') |> 
  report()
```

```{r}
olympic_events_fit |> 
  filter(Length == "10000", Sex == 'women') |> 
  report()
```

```{r}
#| warning: false

olympic_running |> 
  filter(Length == "10000", Sex == "men") |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Men's 10000m Olympic Games times")
```

```{r}
olympic_10000_mens_event_fit <- olympic_events_fit |> 
  filter(Length == "10000", Sex == 'men')

estimates <- olympic_10000_mens_event_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")
```

```{r}
#| warning: false

olympic_running |> 
  filter(Length == "10000", Sex == "women") |>
  autoplot(Time, show.legend = FALSE)  +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE) +
  labs(title = "Women's 10000m Olympic Games times")
```

```{r}
olympic_10000_womens_event_fit <- olympic_events_fit |> 
  filter(Length == "10000", Sex == 'women')

estimates <- olympic_10000_womens_event_fit |> coef() |> select(estimate)
intercept <- estimates[1,1]
slope <- estimates[2,1]

paste0("The regression line is ŷ = ", round(intercept,2), " + ", round(slope,2), "x")
```

Men started competing in the Olympics 10,000m at the start of the 20th century. Their times are, perhaps surprisingly, slower than the women's times who only began competing far more recently in the mid 1980s. Clearly, the culture of modern athleticism has advanced greatly since the start of the 20th century for both sexes.

Due to the initially relatively slow men's Olympic times at 10,000m distances their average rate of improvement is greater, at 0.34, than that of the women's average rate of improvement, at 0.23.

### c

Plot the residuals against the year. What does this indicate about the suitability of the fitted lines?

```{r}
#| warning: false

olympic_10000_mens_event_fit |> gg_tsresiduals()
```

The residuals for the men's 10,000m are fairly small, there is no significant autocorrelation but there is a negitive bias in the results. Is this the median figure of -2.172?

```{r}
#| warning: false

olympic_10000_womens_event_fit |> gg_tsresiduals()
```

The women's 10,000m residuals have fewer points; they are very small indicating that the model has a good accuracy. There is no appreciable autocorrelation. Lastly, the histogram is very roughly normal (it's hard with few so few data points) and there is a small negitive bias: -0.9872.

### d

Predict the winning time for each race in the 2020 Olympics. Give a prediction interval for your forecasts. What assumptions have you made in these calculations?

```{r}
olympic_events_trends_fit <- olympic_running |> 
  model(TSLM(Time ~ trend()))

olympic_events_trends_fc <- olympic_events_trends_fit |> forecast()
olympic_events_trends_fc
```

```{r}
men_10000 <- olympic_running |>
  filter(Length == "10000", Sex == "men")

olympic_events_trends_fc |>
    filter(Length == "10000", Sex == 'men') |> 
  autoplot(men_10000)

olympic_events_trends_fc |> 
  filter(Length == "10000", Sex == 'men') |> 
  hilo()
```

```{r}
women_10000 <- olympic_running |>
  filter(Length == "10000", Sex == "women")

olympic_events_trends_fc |>
    filter(Length == "10000", Sex == 'women') |> 
  autoplot(women_10000)

olympic_events_trends_fc |> 
  filter(Length == "10000", Sex == 'women') |> 
  hilo()
```

Both these predictions are linear meaning that according to the model eventually mankind will be able to travel distances in zero seconds, which is impossible. Hence, better models should be used to capture the point where human performance start to plateau.

## Exercise 3

An elasticity coefficient is the ratio of the percentage change in the forecast variable (y) to the percentage change in the predictor variable (x). Mathematically, the elasticity is defined as (dy/dx)×(x/y). Consider the log-log model,

$$
log \ y=β_0+β_1 \ log \ x+ε
$$

Express y as a function of x and show that the coefficient $β_1$ is the elasticity coefficient.

Taking the derivative of both sides with respect to $x$:

$$
\frac{d}{dx} \ log \ y = \beta_1 \frac{d}{dx} \ log \ x
$$ 
Which becomes:

$$
\frac{1}{y} \ \frac{dy}{dx} = \beta_1 \frac{1}{x}
$$
hence, we can write:

$$
\beta_1 = \frac{dy}{dx} \ \frac{x}{y} 
$$
which is the elasticity coefficient.

## Exercise 4

The data set souvenirs concerns the monthly sales figures of a shop which opened in January 1987 and sells gifts, souvenirs, and novelties. The shop is situated on the wharf at a beach resort town in Queensland, Australia. The sales volume varies with the seasonal population of tourists. There is a large influx of visitors to the town at Christmas and for the local surfing festival, held every March since 1988. Over time, the shop has expanded its premises, range of products, and staff.


```{r}
souvenirs |> head()
```


## a

Produce a time plot of the data and describe the patterns in the graph. Identify any unusual or unexpected fluctuations in the time series.

```{r}
souvenirs |> autoplot(Sales) +
  geom_smooth(formula = "y ~ x", method = "loess", se = FALSE)
```

The time plot shows that since around mid 1990 the shop has started to experience exponential growth.

## b

Explain why it is necessary to take logarithms of these data before fitting a model.

One of the assumptions of linear modelling is that the variance is constant. Clearly this is not the case with the sales data. Using a log transformation of the data helps reduce this invariance issue.

I could have used the Box-Cox transformation to remodel the data, however, as the lambda value is very close to zero it's easier to use a log transformation directly.

```{r}
lambda <- souvenirs |>
  features(Sales, features = guerrero) |>
  pull(lambda_guerrero)
lambda
```


```{r}
value <- 100
# log transformation
paste(log(value))
# box-cox transformation
paste((value^lambda - 1) / lambda)
```
It's not exact but it's close enough.

```{r}
souvenirs |> autoplot(log(Sales))
```

After the log transformation the variance of the sales data now appears constant.

```{r}
dcmp <- souvenirs |>
  model(
    STL(log(Sales) ~ trend(window = 4) + season(window = 12),
    robust = FALSE)) |>
  components() 
dcmp |> autoplot()
```

Transforming the sales data has helped regularise STL decomposition: the remainder resembles faint white noise; season_year has a regular pattern and the trend gradually increases linearly. 

If you use STL decomposition to forecast future sales we end up with the following plot. Notice, how the point forecast continues the same basic pattern as the earlier sales data. The confidence intervals, however, are quite extreme.

```{r}
fit_dcmp <- souvenirs |>
  model(stlf = decomposition_model(
    STL(log(Sales) ~ trend(window = 4) + season(window = 12), robust = FALSE),
    NAIVE(season_adjust)
  ))

fc_dcmp <-forecast(fit_dcmp) 
fc_dcmp |> autoplot(souvenirs)
```


## c

Fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a “surfing festival” dummy variable.

```{r}
new_data <- souvenirs |>
  mutate(festival = ifelse(month(Month) == 3 & year(Month) > 1987, 1, 0))
new_data
```

```{r}
fit <- new_data |>
  model(TSLM(log(Sales) ~ trend() + season() + festival))
report(fit)
```

Including the festival dummy variable increased the `Adjusted R-squared` figure from 0.9447 to 0.9487.

To produce a forecast with the surfing festival dummy variable you need to provide new data covering the time period which includes values for the dummy variable.
```{r}
festival_dates <- rep(0,36)
festival_dates[seq(3,36,12)] = 1
x <- seq(as.Date("1994-01-01"), as.Date("1996-12-31"), by = "1 month")
festival_data <- tsibble(Month = yearmonth(x), festival = festival_dates, index = Month)
festival_data
```


```{r}
souvenirs_fc <- forecast(fit, festival_data) 
souvenirs_fc |> autoplot(souvenirs)
```

```{r}
fit |> gg_tsresiduals()
```

## d

Plot the residuals against time and against the fitted values. Do these plots reveal any problems with the model?

Using this `left_join` method shows the transformed residuals - the scale is between -0.4 to 0.4.

```{r}
souvenirs |>
  left_join(residuals(fit), by = "Month") |>
  ggplot(aes(x = Month, y = .resid)) +
  geom_point() 
```
Alternatively, this way shows the untransformed residuals. Scale -10,000 to 10,000. The transformed residuals appear random whereas there's an obvious pattern in the untransformed residuals.

```{r}
augment(fit) |> 
  ggplot(aes(x=Month)) +
  geom_point(aes( y=.resid))
```

The fitted values against the residuals looks to be very clustered with a handful of outliers which would indicate that the modelling assumptions have not been satisfied.

```{r}
augment(fit) |> 
  ggplot(aes(x=.fitted)) +
  geom_point(aes(y=.resid))
```
## e

Do boxplots of the residuals for each month. Does this reveal any problems with the model?

```{r}
augment(fit) |> 
  index_by(Date = month(Month)) |> 
  ggplot(aes(x = factor(month.abb[Date], month.abb), y = .resid)) +
  geom_boxplot() + labs(x="Month", y = "Residuals")
```
The residuals in the final quarter have increasing trend and variance. December's variance is particularly large.

```{r}
souvenirs |>
  left_join(residuals(fit), by = "Month") |> 
  index_by(Date = month(Month)) |> 
  ggplot(aes(x = factor(month.abb[Date], month.abb), y = .resid)) +
  geom_boxplot() + labs(x="Month", y = "Residuals")
```
In contrast the transformed residuals have means that fluctuate more closely around 0. Their variances are also more homogeneous.


## f

What do the values of the coefficients tell you about each variable?

```{r}
coefficients <- coef(fit) |> select(term, estimate)
coefficients
```

The coefficients show that there is a small upward trend in sales. Seasonally speaking, the main sales period is in the last quarter. From the start of the year, sales begin weakly but progressively build towards the second and third quarters. Also, the surfer festival period generates a significant boost in sales during the quieter sales period. Compare with the raw box plots above.

## g

What does the Ljung-Box test tell you about your model?


```{r}
fit |> augment() |> features(.innov, ljung_box, lag = 10)
```
The Ljung-Box test has a p-value significantly lower than 0.05 meaning that the residuals aren't random, i.e. they cannot be considered as white noise and therefore the model doesn't explain all the uncertainty contained in the data.

## h

Regardless of your answers to the above questions, use your regression model to predict the monthly sales for 1994, 1995, and 1996. Produce prediction intervals for each of your forecasts.

```{r}
souvenirs_fc <-  forecast(fit, festival_data) 
souvenirs_fc |> 
  autoplot(souvenirs)
```


```{r}
souvenirs_fc |> hilo() |> select(`80%`, `95%`)
```

## i

How could you improve these predictions by modifying the model?

The ACF plot shows there is a lot of autocorrelation at lag 1 which suggests that the data are not stationary. Making the data stationary first, by calculating diff 1, may lead to a better model.


## Exercise 5

The us_gasoline series consists of weekly data for supplies of US finished motor gasoline product, from 2 February 1991 to 20 January 2017. The units are in “million barrels per day”. Consider only the data to the end of 2004.

```{r}
us_gasoline_data <- us_gasoline |> 
  filter(year(Week) < "2005") 

us_gasoline_data |> autoplot(Barrels)
```
## a

Fit a harmonic regression with trend to the data. Experiment with changing the number Fourier terms. Plot the observed gasoline and fitted values and comment on what you see.

```{r}
gasoline_fit <- us_gasoline_data |>
  model(TSLM(Barrels ~ trend() + fourier(K = 7)))
report(gasoline_fit)
```

```{r}
augment(gasoline_fit) |> 
  ggplot(aes(x=Barrels)) +
  geom_point(aes(y=.fitted))
```
The plot shows a linear relationship with a reasonable spread of variance. There are no outliers.

Select the appropriate number of Fourier terms to include by minimising the AICc or CV value.

```{r}
glance(gasoline_fit) |> select(AIC, AICc, CV)
```
K = 4  AICc -1856.304	 CV  0.07727546
K = 6  AICc -1883.542	 CV  0.07441244	
K = 7  AICc -1887.354	 CV  0.07401329	
K = 8  AICc -1884.573	 CV  0.0742856	
K = 10 AICc -1881.415  CV  0.07458137		

K = 7 is the produces the best AICc and CV results. Remember lower numbers for AICc and CV are better.

## c

Plot the residuals of the final model using the gg_tsresiduals() function and comment on these. Use a Ljung-Box test to check for residual autocorrelation.

```{r}
gasoline_fit |> gg_tsresiduals()
```

```{r}
gasoline_fit |> augment() |> features(.innov, ljung_box, lag = 10)
```

The Ljung-Box p-value is just less than 0.05 meaning that the residuals cannot be considered as white noise, however, the p-value is very close to the threshold value meaning the model is fairly good but could still be improved upon.



## d

Generate forecasts for the next year of data and plot these along with the actual data for 2005. Comment on the forecasts.

```{r}

us_gasoline_2005 <- us_gasoline |> filter(year(Week) == "2005")

forecast(gasoline_fit, h = 52) |>
  autoplot() +
  autolayer(us_gasoline_2005, Barrels)
```
For the most part the model predicts the number of barrels supplied although in the third quarter of 2005 the number dropped dramatically, recovering slowly over the quarter.

# Exercise 6

The annual population of Afghanistan is available in the global_economy data set.

```{r}
population <- global_economy |> 
  filter(Country == "Afghanistan") |> 
  select(Population) 

population |> 
  autoplot(Population) +
  geom_smooth(formula = "y~x", method="lm")
```

The Soviet-Afghan War took place from 1979 to 1989 which coincides with the dip in the Population data. A linear model tracks the population data very poorly.

## b

Fit a linear trend model and compare this to a piecewise linear trend model with knots at 1980 and 1989.


```{r}
population_fit <- population |> 
  model(
    linear = TSLM(Population ~ trend()),
    piecewise = TSLM(Population ~ trend(knots = c(1980, 1989)))
    )
glance(population_fit) |> select(.model, AICc, CV)
```

The AICc and CV statistics clearly show that the piecewise model is the better of the two choices.

## c

Generate forecasts from these two models for the five years after the end of the data, and comment on the results.

```{r}
population_fc <- population_fit |> forecast(h = 5)

population |> 
  autoplot(Population) +
  geom_line(data = fitted(population_fit),
            aes(y = .fitted, colour = .model)) +
  autolayer(population_fc, alpha = 0.5, level = 95) +
  labs(title = "Afghanistan Population Projections", y = "Population")
```
Clearly, the piecewise model provides more realistic results.

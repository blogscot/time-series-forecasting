---
title: "Time Series"
author: "Iain Diamond"
format: 
  html:
    code-fold: true
---

For when I get stuck [Taha Ahmad Solutions](https://rstudio-pubs-static.s3.amazonaws.com/1085722_6a584602005b456e87d51f7b477a4bf8.html)

## Chapter 5 Exercises

```{r}
# load the libraries
library(fpp3)
```

# Exercise 1

Produce forecasts for the following series using whichever of NAIVE(y), SNAIVE(y) or RW(y \~ drift()) is more appropriate in each case:

## Australian Population (global_economy)

```{r}
aus_population <- global_economy |> 
  filter(Country == "Australia")
  
aus_population |> 
  autoplot(Population)
```

I included the graph below for my own curiosity and to learn about scaling.

```{r}
autoplot(aus_population, scale(GDP/Population), color="black", linetype="dashed") +
  autolayer(aus_population, scale(Population), color="blue") +
  labs(title="Australian Population, GDP per capita (dashed)",
       y="Scaled Population, GDP per capita")
```

```{r}
aus_population_fc <- aus_population |> 
  model(Drift = NAIVE(Population ~ drift())) |> 
  forecast(h = "10 years")

aus_population_fc |>
  autoplot(aus_population, level = 90) +
  labs(title = "Australian Population Forecast")
```

## Bricks (aus_production)

```{r}
#| warning: false

bricks <- aus_production |> 
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Bricks) 

bricks |> autoplot(Bricks)
```

```{r}
aus_brick_fc <- bricks |> 
  model(Seasonal = SNAIVE(Bricks ~ lag("4 years"))) |> 
  forecast(h = 20)

aus_brick_fc |>
  autoplot(bricks, level = 80) +
  labs(title = "Australian Brick Production Forecast")
```

## NSW Lambs (aus_livestock)

```{r}
lambs <- aus_livestock |> filter(Animal == "Lambs" & State == "New South Wales")
lambs |> autoplot(Count)
```

```{r}
aus_lamb_fc <- lambs |> 
  model(Seasonal = SNAIVE(Count ~ lag("10 years"))) |> 
  forecast(h = 20)

aus_lamb_fc |>
  autoplot(lambs, level = 85) +
  labs(title = "Australian Lamb Forecast")
```

## Household wealth (hh_budget)

```{r}
wealth <- hh_budget |> select(Wealth)
wealth |> autoplot(Wealth)
```

```{r}
wealth_fc <- wealth |> 
  model(Drift = NAIVE(Wealth ~ drift())) |> 
  forecast(h = "5 years")

wealth_fc |>
  autoplot(wealth, level = 90) +
  labs(title = "Wealth Forecast")
```

## Australian takeaway food turnover (aus_retail)

```{r}
takeout <- aus_retail |> filter(Industry == 'Takeaway food services')
takeout
```

```{r}
takeout_turnover <- takeout |> 
  group_by(Industry) |> 
  summarise(Turnover = sum(Turnover))

takeout_turnover |> autoplot(Turnover) +
  labs(title = "Australian Takeaway Food Services")
  
```

The variance is increasing over time. Time for a transformation!

```{r}
lambda <- takeout_turnover |> 
  features(Turnover, features = guerrero) |> pull()
lambda
```

This lambda value is fairly close to zero, so a log transform should balance the variance well enough to aid in forecasting.

```{r}
takeout_turnover |> autoplot(log(Turnover)) +
  labs(title = "Australian Takeaway Food Services")
```

As I understand it transforming the Turnover to reduce the variance helps with forecasting. The results are untransformed in the background automatically by the forecast package.

```{r}
takeout_fc <- takeout_turnover |> 
  model(SNAIVE(log(Turnover) ~ lag("2 years"))) |> 
  forecast(h = "5 years")
takeout_fc |> relocate(.model)
```

```{r}
takeout_fc |>
  autoplot(takeout_turnover, level = 90) +
  labs(title = "Takeaway Food Services Forecast")
```

# Exercise 2

Use the Facebook stock price (data set gafa_stock) to do the following:

Produce a time plot of the series.

```{r}
fb_stock <- gafa_stock |> 
  filter(Symbol == "FB") |> 
  mutate(day = row_number()) |> 
  update_tsibble(key = Symbol, index = day, regular = TRUE)  

gafa_stock |> 
  filter(Symbol == "FB") |> 
  autoplot(Close)

```

Produce forecasts using the drift method and plot them.

```{r}
fb_stock_training <- fb_stock |> 
  filter(yearmonth(Date) < yearmonth("2018 Jan"))

fb_stock_test <-  fb_stock |> 
  filter(yearmonth(Date) >= yearmonth("2018 Jan"))

fb_stock_training |> 
  autoplot(Close)
```

Show that the forecasts are identical to extending the line drawn between the first and last observations.

```{r}
fb_stock_fc <- fb_stock_training |>
  model(Drift = NAIVE(Close ~ drift())) |> 
  forecast(new_data = fb_stock_test) 
```

```{r}
#| message: false

fb_first <- fb_stock_training |> slice_head(n=1)
fb_last <- fb_stock_training |> slice_tail(n=1)

fb_stock_fc |>
  autoplot(fb_stock_training, level = c(80, 95)) +
  autolayer(fb_stock_test, colour = "grey") +
  geom_segment(aes(x=fb_first$day, y=fb_first$Close, xend=fb_last$day, yend=fb_last$Close), colour = "blue", linetype="dashed") +
  labs(title = "Facebook Stock Forecast") + 
  labs(y = "$US",
       title = "Facebook daily closing stock prices") +
  guides(colour = guide_legend(title = "Forecast"))
```

The forecast produced by the drift benchmark, as this graph illustrates, offers very poor accuracy given the sudden change in stock values immediately following the training period. Given the benefit of hindsight, a naïve model (using the last closing stock price) would have been more accurate, while a mean model would have provided the best results.

## Exercise 3

Apply a seasonal naïve method to the quarterly Australian beer production data from 1992. Check if the residuals look like white noise, and plot the forecasts.

```{r}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)

recent_production |> 
  autoplot(Beer)
```

```{r}
#| warning: false

# Define and estimate a model
fit <- recent_production |> model(SNAIVE(Beer))

# Look at the residuals
fit |> gg_tsresiduals()
```

The ACF of the residuals shows a strong correlation around 1 year which reflects the seasonality of the Beer time plot. The histogram of the residuals as zero mean and is not quite normally distributed. It is in fact bi-modal.

```{r}
fit |> forecast() |> autoplot(recent_production)
```

```{r}
#| warning: false

aug <- recent_production |>
  model(SNAIVE(Beer)) |>
  augment()

autoplot(aug, .innov) +
  labs(y = "Beer",
       title = "Residuals from the naïve method")
```

```{r}
aug |> features(.innov, box_pierce, lag = 8)
```

```{r}
aug |> features(.innov, ljung_box, lag = 8)
```

Both the Box-Pierce and Ljung-Box values are significant indicating we can accept the null hypothesis that the results are distinguishable from white noise. This suggests that there remains unexplained information in the model.

```{r}
recent_production |>
  model(
    STL(Beer ~ trend(window = 4),
    robust = FALSE)) |>
  components() |>
  autoplot()
```

Decomposing the time plot shows there is a gradual negative trend in beer consumption with dips around 1996, 2005 and 2007.

## Exercise 4

Repeat the previous exercise using the Australian Exports series from global_economy and the Bricks series from aus_production. Use whichever of NAIVE() or SNAIVE() is more appropriate in each case.

```{r}
aus_exports <- global_economy |> 
  filter(Country == "Australia") |> 
  select(Exports)

aus_exports |> 
  autoplot(Exports)
```

```{r}
#| warning: false
fit <- aus_exports |> model(NAIVE(Exports)) 

fit |> gg_tsresiduals()
```

```{r}
fit |> forecast() |> autoplot(aus_exports)
```

```{r}
#| warning: false

aug <- aus_exports |>
  model(NAIVE(Exports)) |>
  augment()

aug |> features(.innov, box_pierce, lag = 10)
```

```{r}
aug |> features(.innov, ljung_box, lag = 10)
```

Both Box-Pierce and Ljung-Box p-values are above the arbitrary threshold of 5% which means that we can reject the null hypothesis: these data are indistinguishable from white noise.

```{r}
#| warning: false

bricks <- aus_production |> 
  select(Bricks) |> 
  filter(!is.na(Bricks))

bricks |> autoplot(Bricks)
```

```{r}
#| warning: false

# Define and estimate a model
fit <- bricks |> model(SNAIVE(Bricks ~ lag("year")))

# Look at the residuals
fit |> gg_tsresiduals()
```

The baseline SNAIVE model shows lots of problems. Analysis of the residuals has a range of -200 to 100 which is far too high. The ACF plot shows large amounts of autocorrelation and the histogram is right-skewed with non-zero mean.

```{r}
fit |> forecast() |> autoplot(bricks)
```

```{r}
bricks |>
    model(seats = X_13ARIMA_SEATS(Bricks ~ seats())) |>
  components() |>
  autoplot()
```

Contrast the SNAIVE model's residuals with the irregular data produced using a 13ARMIMA_SEATS model.

## Exercise 5

Produce forecasts for the 7 Victorian series in aus_livestock using SNAIVE(). Plot the resulting forecasts including the historical data. Is this a reasonable benchmark for these series?

```{r}
aus_livestock |> 
  filter(State == 'Victoria') |>
  autoplot(Count)
```

### Bull Production

```{r}
bull_production <- aus_livestock |> 
  filter(State == 'Victoria' & Animal == "Bulls, bullocks and steers") 
bull_production |> autoplot(Count)
```

```{r}
#| warning: false

fit <- bull_production |> model(SNAIVE(Count ~ lag("2 years"))) 
fit |> gg_tsresiduals()
```

```{r}
fit |> forecast() |> autoplot(bull_production)
```

### Sheep Production

```{r}
sheep_production <- aus_livestock |> 
  filter(State == 'Victoria' & Animal == "Sheep") 
sheep_production |> autoplot(Count)
```

```{r}
#| warning: false

sheep_production |> model(SNAIVE(Count)) |> gg_tsresiduals()
```

```{r}
#| warning: false

# Define and estimate a model
fit <- sheep_production |> model(SNAIVE(log(Count)))
fit |> gg_tsresiduals()
```

```{r}
fit |> forecast() |> autoplot(sheep_production)
```

Overall, these benchmark forecasts are unlikely to be very accurate, however, they do provide a reasonable 'sane' set of results upon which we could endeavour to improve.

## Exercise 7

```{r}
myseries <- aus_retail |> filter(`Series ID` == 'A3349763L')
myseries |> head()
```

Create a training dataset consisting of observations before 2011

```{r}
myseries_train <- myseries |> filter(year(Month) < 2011)
```

Check that your data have been split appropriately.

```{r}
autoplot(myseries, Turnover) +
  autolayer(myseries_train, Turnover, colour = "red", )
```

Fit a seasonal naïve model using SNAIVE() applied to your training data (myseries_train)

```{r}
#| warning: false

fit <- myseries_train |> model(SNAIVE(Turnover ~ lag("3 years")))
fit |> gg_tsresiduals()
```

```{r}
fc <- fit |> forecast(new_data = anti_join(myseries, myseries_train))
fc |> autoplot(myseries)
```

Without even having to look at the accuracy statistics the graph above shows how arbitrary the forecast is. When the Turnover spikes in the mid-2010s the forecast is significantly in error. It is only because the Turnover figures happen to fall that the forecast statistics appear reasonable at all.

```{r}
fit |> accuracy() |> select(c(-State,-Industry, -.model, -.type))
```

```{r}
fc |> accuracy(myseries) |> select(c(-State,-Industry, -.model, -.type))
```

The accuracy figures for the forecast look better than the actually are, as the forecast mismatch against actual turnover in the graph above demonstrates. If the forecast line actually tracked the actual turnover line then these figures might have some real significance. It is imperative to see how the forecast model performs visually to ascertain what level of confidence to give to the statistical values produced in terms of accuracy.

## Exercise 8

Consider the number of pigs slaughtered in New South Wales (data set aus_livestock).

```{r}
pigs <- aus_livestock |> filter(State == 'New South Wales', Animal == 'Pigs') 
pigs
```

Create a training set of 486 observations, withholding a test set of 72 observations (6 years).

```{r}
pigs_training <- pigs |> slice(0:486)
pigs_training
pigs_test <- pigs |> slice(487:n())
pigs_test

autoplot(pigs_training, Count) +
  autolayer(pigs_test, Count, colour = 'red')
```

Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

```{r}
#| warning: false
#| message: false

fit1 <- pigs_training |> model(SNAIVE(Count))
fit1 |> gg_tsresiduals()

fc <- fit1 |> forecast(new_data = pigs_test)
fc |>  autoplot(pigs)
```

```{r}
#| warning: false
#| message: false

fit2 <- pigs_training |> model(MEAN(Count))
fit2 |> gg_tsresiduals()

fc <- fit2 |> forecast(new_data = pigs_test)
fc |>  autoplot(pigs)
```

```{r}
#| warning: false
#| message: false

fit3 <- pigs_training |> model(NAIVE(Count))
fit3 |> gg_tsresiduals()

fc <- fit3 |> forecast(new_data = pigs_test)
fc |>  autoplot(pigs)
```

```{r}
#| warning: false
#| message: false

fit4 <- pigs_training |> model(NAIVE(Count ~ drift()))
fit4 |> gg_tsresiduals()

fc <- fit4 |> forecast(new_data = pigs_test)
fc |>  autoplot(pigs)
```

```{r}
fit1 |> accuracy() |> select(c(-Animal, -State, -.model, -.type))
fit2 |> accuracy() |> select(c(-Animal, -State, -.model, -.type))
fit3 |> accuracy() |> select(c(-Animal, -State, -.model, -.type))
fit4 |> accuracy() |> select(c(-Animal, -State, -.model, -.type))
```

## Exercise 11

We will use the Bricks data from aus_production (Australian quarterly clay brick production 1956--2005) for this exercise.

```{r}
bricks <- aus_production |> 
  select(Bricks) |> 
  filter_index("1956 Q1" ~ "2005 Q2")
bricks
```

```{r}
bricks |> 
  autoplot(Bricks) +
  geom_smooth(formula = "y ~ x", method = "loess", se = FALSE)
```

Use an STL decomposition to calculate the trend-cycle and seasonal indices. (Experiment with having fixed or changing seasonality.)

```{r}
brick_components <- bricks |>   
  model(STL(Bricks ~ trend(window = 4) + season(window = "periodic"), robust = FALSE)) |>
  components() 

brick_components |> autoplot(Bricks)
```

Using a 'periodic' season window produces a very stable season_year plot while the remainder looks like random noise. Slightly narrower ranges for remainder are possible but at the cost of destablising the season_year plot. Which is more significant?

Use a naïve method to produce forecasts of the seasonally adjusted data.

```{r}
dcmp <- brick_components |> select(-.model)

dcmp |>
  model(SNAIVE(season_adjust ~ lag("4 years"))) |>
  forecast() |>
  autoplot(dcmp) +
  labs(y = "Bricks (million)",
       title = "Australian Brick Production")
```

```{r}
fit_dcmp <- bricks |> 
  model(stlf = decomposition_model(
    STL(Bricks ~ trend(window = 4) + season(window = "periodic"), robust = TRUE),
    NAIVE(season_adjust)
  ))

dcmp_fc <- fit_dcmp |> forecast(h = 10)
dcmp_fc
```

```{r}
dcmp_fc |>
  autoplot(bricks)+
  labs(y = "Bricks",
       title = "Australian Brick Production")
```

Do the residuals look uncorrelated?

```{r}
#| warning: false

fit_dcmp |> gg_tsresiduals()
```

There is some correlation in the residuals.

Repeat with a robust STL decomposition. Does it make much difference?

No, it doesn't make any observable difference.

Compare forecasts from decomposition_model() with those from SNAIVE(), using a test set comprising the last 2 years of data. Which is better?

```{r}
bricks_test <- aus_production |> 
  select(Bricks) |> 
  filter_index("2003 Q1" ~ "2005 Q2")

dcmp2 <- bricks_test |>   
  model(STL(Bricks ~ trend(window = 2), robust = FALSE)) |>
  components() |> 
  select(-.model)

dcmp2 |>
  model(SNAIVE(season_adjust ~ lag("2 years"))) |>
  forecast() |>
  autoplot(dcmp) +
  labs(y = "Number of Bricks (million)",
       title = "Australian Production")
```

The latter model has narrower confidence levels which suggests that this model's predictions should be more accurate. However, this model is basing its forecasts only on the last 2 years of data, ignoring the earlier historic data. Is this really a reasonable assumtpion to make - I very much doubt it.

---
title: "Chp 10.qmd"
author: "Iain Diamond"
format: 
  html:
    toc: true
    code-fold: true
---

# Chapter 10

For when I get stuck [Tugas Kelompok](https://rpubs.com/invokerarts/UAS_Eco)

```{r}
library(fpp3)
```

## Exercise 1

This exercise uses data set LakeHuron giving the level of Lake Huron from 1875--1972.

### a

Convert the data to a tsibble object using the as_tsibble() function.

```{r}
LakeHuron
```

```{r}
lake_huron <- LakeHuron |> as_tsibble()
lake_huron |> autoplot(value)
```

### b

Fit a piecewise linear trend model to the Lake Huron data with a knot at 1920 and an ARMA error structure.

```{r}
lake_fit <- lake_huron |> model(ARIMA(value ~ trend(knot = 1920)))
lake_huron |>
  autoplot(value) +
  geom_line(data = fitted(lake_fit),
            aes(y = .fitted),
            colour = "#ffaa44",
            show.legend = FALSE)
```

### c

Forecast the level for the next 30 years. Do you think the extrapolated linear trend is realistic?

```{r}
lake_fc <- lake_fit |> forecast(h = 30)
lake_huron |>
  autoplot(value) +
  geom_line(data = fitted(lake_fit),
            aes(y = .fitted),
            colour = "#ffaa44") +
  autolayer(lake_fc, alpha = 0.5)
```

The ARIMA point estimate forecast looks accurate only for first few years. Later however, the model seems to give up and settle on a mean value for longer term forecasts.

## Exercise 2

Repeat Exercise 4 from Section 7.10, but this time adding in ARIMA errors to address the autocorrelations in the residuals.

### a

How much difference does the ARIMA error process make to the regression coefficients?

Firstly, applying log to deal with the non-constant variance.

```{r}
souvenirs |> autoplot(log(Sales))
```

```{r}
new_data <- souvenirs |>
  mutate(festival = ifelse(month(Month) == 3 & year(Month) > 1987, 1, 0))
new_data
```

```{r}
souvenirs_tslm_fit <- new_data |>
  model(TSLM(log(Sales) ~ festival + trend() + season()))
report(souvenirs_tslm_fit)
```

```{r}
souvenirs_arima_fit <- new_data |> 
  model(ARIMA(log(Sales) ~ festival + trend() + season()))
report(souvenirs_arima_fit)
```

There are quite a few similarities in the intercept, festival, trend and seasonal values.

|           | TSLM  | ARIMA |
|-----------|-------|-------|
| intercept | 7.62  | 7.6   |
| festival  | 0.5   | 0.46  |
| trend     | 0.022 | 0.022 |

### b

How much difference does the ARIMA error process make to the forecasts?

```{r}
souvenirs_tslm_fit |> accuracy()
souvenirs_arima_fit |> accuracy()
```

The ARIMA model is significantly more accurate.

To produce a forecast with the surfing festival dummy variable you need to provide new data covering the time period which includes values for the dummy variable.

```{r}
festival_dates <- rep(0,36)
festival_dates[seq(3,36,12)] = 1
x <- seq(date("1994-01-01"), by = "month", length.out = 36)
festival_data <- tsibble(Month = yearmonth(x), festival = festival_dates, index = Month)
festival_data
```

```{r}
forecast(souvenirs_arima_fit, festival_data) |> autoplot(souvenirs |> tail(24))
```

### c

Check the residuals of the fitted model to ensure the ARIMA process has adequately addressed the autocorrelations seen in the TSLM model.

```{r}
resid(souvenirs_tslm_fit) |> 
  gg_tsdisplay(.resid, plot_type = "partial")
```

```{r}
resid(souvenirs_arima_fit) |> 
  gg_tsdisplay(.resid, plot_type = "partial")
```

The ARIMA model shows no significant autocorrelation in the ACF, and PACF plots unlike the TSLM model's.

## Exercise 3

Repeat the daily electricity example, but instead of using a quadratic function of temperature, use a piecewise linear function with the "knot" around 25 degrees Celsius (use predictors Temperature & Temp2). How can you optimise the choice of knot?

Using a function to generate data with different knot values makes life so much easier.

```{r}
generate_data <- function(knot) {
  vic_elec |>
    filter(year(Time) == 2014) |>
    index_by(Date = date(Time)) |>
    summarise(
      Demand = sum(Demand)/1e3,
      Temperature = max(Temperature),
      Holiday = any(Holiday)) |>
    mutate(
      Temp2 = I(pmax(Temperature-knot,0)),
      Day_Type = case_when(
        Holiday ~ "Holiday",
        wday(Date) %in% 2:6 ~ "Weekday",
        TRUE ~ "Weekend"))
}

vic_elec_daily <- generate_data(27)

vic_elec_daily |>
  ggplot(aes(x = Temperature, y = Demand, colour = Day_Type)) +
  geom_point() +
  labs(y = "Electricity demand (GW)",
       x = "Maximum daily temperature")
```

```{r}
vic_elec_daily |>
  pivot_longer(c(Demand, Temperature, Temp2)) |>
  ggplot(aes(x = Date, y = value)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") + ylab("")
```

```{r}
vic_elec_fit <- vic_elec_daily |> 
  model(ARIMA(log(Demand) ~ Temperature + 
                Temp2 + 
                (Day_Type == "Weekday"),
                stepwise = FALSE,
                greedy = TRUE,
                order_constraint = p+q+P+Q <= 10))
report(vic_elec_fit)
```

After much experimentation the ARIMA model:

Model: LM w/ ARIMA(5,1,3)(2,0,0)\[7\] errors

was found to produce residuals that look like white noise.

```{r}
vic_elec_fit |> gg_tsresiduals()
```

```{r}
vic_elec_fit |>  
  augment() |> 
  features(.innov, ljung_box, dof = 4, lag = 10) |> 
  pull(lb_pvalue)
```

To optimise the knot, I've used the trial and error method, by simply plugging in values into the generate_data function using the _best_ ARIMA model. 

Note, it is far faster to tweak the knot value having established the ARIMA model compared with having the ARIMA algorithm search for the best fit model during each run.

```{r}
vic_elec_fit2 <- generate_data(27) |> 
  model(ARIMA(log(Demand) ~ Temperature + 
                Temp2 + 
                (Day_Type == "Weekday") +
                1 + pdq(5,1,3) + PDQ(2,0,0)))
report(vic_elec_fit2)
```
Some results:

| knot | AICc     |
|------|----------|
| 20   | -1411.8  |
| 25   | -1417.3  |
| 27   | -1418.45 |
| 28   | -1417.74 |
| 30   | -1397.78 |


Just to confirm that after finding the best knot value the residuals are still well behaved.

```{r}
vic_elec_fit2 |> gg_tsresiduals()
```
```{r}
vic_elec_fit |>  
  augment() |> 
  features(.innov, ljung_box, dof = 4, lag = 10) |> 
  pull(lb_pvalue)
```

Out of curiousity, I fitted the electricity data without applying a log transformation in order to compare its affect on accuracy. The AICc values are wildly different between using a transformation and not using a transformation, but just how significant is it?


```{r}
vic_elec_fit3 <- generate_data(27) |> 
  model(ARIMA(Demand ~ Temperature + 
                Temp2 + 
                (Day_Type == "Weekday") +
                1 + pdq(5,1,3) + PDQ(2,0,0)))
report(vic_elec_fit3)
```
```{r}
acc2 <- vic_elec_fit2 |> accuracy() |> select(RMSE, MAE, MAPE)
acc2
```

```{r}
acc3 <- vic_elec_fit3 |> accuracy() |> select(RMSE, MAE, MAPE)
acc3
```

The accuracy improvements between models due to use the log transformation, as percentages:

```{r}
round((acc3$RMSE - acc2$RMSE) / acc3$RMSE * 100, 2)
round((acc3$MAE - acc2$MAE) / acc3$MAE * 100, 2)
round((acc3$MAPE - acc2$MAPE) / acc3$MAPE * 100, 2)
```
